{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890db270",
   "metadata": {},
   "source": [
    "What is Web Scrapping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caf40a8",
   "metadata": {},
   "source": [
    "Web scrapping is simply a way of obtaining large amounts of data from websites on the internet. This is an automated approach, as opposed to copying and pasting the data you want from a website (because gosh, what if you want 100s of data points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6329032b",
   "metadata": {},
   "source": [
    "What is happening under the hood?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e2c776",
   "metadata": {},
   "source": [
    "Web scrapping involves what we call a scrapper in this case that is our python script/Program. So there are the steps:\n",
    "\n",
    "1. The scrapper (python program) sends a request to a website and asks for a specific page-eg, give me the air fryer page on jumia \n",
    "2. The websites send back the page\n",
    "3. Data extraction-you, the programmer have coded the scrapper to parse through the (returned) 'page' for specific data. Say, you only want the prices from the airfyer page on Jumia and nothing else . \n",
    "4. Clean, Organize and store data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a5de60",
   "metadata": {},
   "source": [
    "When you run the request.get() you get a response object as output.This object contains\n",
    "1. status_code (404, 500 error)\n",
    "2. text(HTML format)\n",
    "3. Content \n",
    "4. Headers (Who make the request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f4dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "#run the code pip install requests \n",
    "#run the code pip install beautifulsoup4\n",
    "#read on cellenium documentation for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09907c54",
   "metadata": {},
   "source": [
    "The path of the information is found "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daa3a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrapping the name and the prices of the products\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "654e029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the URL you want to scrap\n",
    "url = 'https://www.jumia.co.ke/catalog/?q=air+fryer'\n",
    "\n",
    "#create a fake user- so that you do not get blocked \n",
    "\n",
    "headers = {\n",
    "    \n",
    "    \"User-Agent\": \"Chrome/5.0\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed318d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now send a get request\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "#then use the html parser to read the content off the web page \n",
    "#Parsing the HTML \n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "print(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4995ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = soup.find_all('article', class_='prd _fb col c-prd')\n",
    "products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0231314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through the products and extract the name and price \n",
    "\n",
    "for product in products:\n",
    "    name = product.find(\"h3\", class_=\"name\")\n",
    "    new_price = product.find(\"div\", class_=\"prc\")\n",
    "    old_price = product.find(\"div\", class_=\"old\")\n",
    "\n",
    "    if name and new_price:\n",
    "        print(f\"Product: {name.text.strip()}\")\n",
    "        print(f\"New Price: {new_price.text.strip()}\")\n",
    "        if old_price:\n",
    "            print(f\"Old Price: {old_price.text.strip()}\")\n",
    "        else:\n",
    "            print(\"Old Price: Not listed\")\n",
    "        print(\"-\" * 40)\n",
    "#strip() is used to remove any whitespaces characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "787b23b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the data on an excel file or a csv file \n",
    "\n",
    "import csv\n",
    "\n",
    "with open(\"air_fryers_jumia.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Product_name\", \"New_Price\", \"Old_Price\"])\n",
    "\n",
    "    for product in products:\n",
    "        name = product.find(\"h3\", class_=\"name\")\n",
    "        new_price = product.find(\"div\", class_=\"prc\")\n",
    "        old_price = product.find(\"div\", class_=\"old\")\n",
    "\n",
    "        if name and new_price:\n",
    "            product_name=name.text.strip()\n",
    "            current_price=new_price.text.strip()\n",
    "            if old_price:\n",
    "                original_price=old_price.text.strip()\n",
    "            else:\n",
    "                original_price=\"Not listed\"\n",
    "                writer.writerow([product_name, current_price, original_price])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1a6ab9",
   "metadata": {},
   "source": [
    "Scrapping for various pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccce6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "\n",
    "with open(\"air_fryers_multi_page2.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Product Name\", \"New Price\", \"Old Price\"])  # CSV Header\n",
    "\n",
    "    # We modify our code to loop through all the pages and scrap, all else remains the same\n",
    "    for page in range(1, 7):  # You can increase this to 10 or more\n",
    "        #print(f\"Scraping Page {page}...\")\n",
    "\n",
    "        url = f\"https://www.jumia.co.ke/catalog/?q=air+fryer&page={page}\"\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        products = soup.find_all(\"article\", class_=\"prd\")\n",
    "\n",
    "        for product in products:\n",
    "            name = product.find(\"h3\", class_=\"name\")\n",
    "            new_price = product.find(\"div\", class_=\"prc\")\n",
    "            old_price = product.find(\"div\", class_=\"old\")\n",
    "\n",
    "            if name and new_price:\n",
    "                product_name = name.text.strip()\n",
    "                current_price = new_price.text.strip()\n",
    "                original_price = old_price.text.strip() if old_price else \"Not listed\"\n",
    "\n",
    "                writer.writerow([product_name, current_price, original_price])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maurineenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
